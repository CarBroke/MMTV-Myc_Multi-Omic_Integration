{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "843b25a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ANDREC~1\\AppData\\Local\\Temp/ipykernel_34896/3603505079.py:88: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df1['END'] = df1['INFO'].str.extract(';END=(\\d+)')\n",
      "C:\\Users\\ANDREC~1\\AppData\\Local\\Temp/ipykernel_34896/3603505079.py:89: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df1['POS'] = df1['POS'].astype(int)\n",
      "C:\\Users\\ANDREC~1\\AppData\\Local\\Temp/ipykernel_34896/3603505079.py:90: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df1['END'] = df1['END'].astype(int)\n",
      "C:\\Users\\ANDREC~1\\AppData\\Local\\Temp/ipykernel_34896/3603505079.py:91: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df1['LENGTH'] = df1['END'] - df1['POS']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# Full Python Script\n",
    "# Python 3.8.8\n",
    "# Pandas 1.2.4\n",
    "# Numpy 1.20.1\n",
    "# Carson Broeker, 10/19/2021\n",
    "#Test script for converting Delly and Lumpy CNV vcf data to a list of genes\n",
    "# First, import necessary modules\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import csv\n",
    "# Make it so really large files don't cause the system to error out\n",
    "maxInt = sys.maxsize\n",
    "\n",
    "while True:\n",
    "    # decrease the maxInt value by factor 10 \n",
    "    # as long as the OverflowError occurs.\n",
    "\n",
    "    try:\n",
    "        csv.field_size_limit(maxInt)\n",
    "        break\n",
    "    except OverflowError:\n",
    "        maxInt = int(maxInt/10)\n",
    "# Next, read in all of the VCF files from Delly and Lumpy using a for loop\n",
    "# This step also gets rid of superfluous columns for downstream analyses\n",
    "# Modify the List elements to suit your project needs\n",
    "# Make sure all of your vcf files are in the same directory/location as this python script\n",
    "#dfFVBL = pd.read_csv(\"FVB_NJLumpy.vcf\", sep='\\t', skiprows=32, usecols=[0,1,4,7], engine = 'c', dtype=object)\n",
    "#dfFVBL = dfFVBL[(dfFVBL['ALT'] == '<DUP>') | (dfFVBL['ALT'] == '<DEL>')]\n",
    "#dfFVBL['END'] = dfFVBL['INFO'].str.extract(';END=(\\d+)')\n",
    "#dfFVBL['POS'] = dfFVBL['POS'].astype(int)\n",
    "#dfFVBL['END'] = dfFVBL['END'].astype(int)\n",
    "#dfFVBL['LENGTH'] = dfFVBL['END'] - dfFVBL['POS']\n",
    "#dfFVBL = dfFVBL.sort_values(by=['POS'])\n",
    "#dfFVBL = dfFVBL[dfFVBL['LENGTH'] > 10000]\n",
    "#dfFVBL = dfFVBL[dfFVBL['LENGTH'] < 100000000]\n",
    "#dfFVBL['SU'] = dfFVBL['INFO'].str.extract(';SU=(\\d+)')\n",
    "#dfFVBL['SU'] = dfFVBL['SU'].astype(int)\n",
    "#dfFVBL = dfFVBL[dfFVBL['SU'] > 4]\n",
    "#dfFVBL['CIPOS'] = dfFVBL['INFO'].str.extract(';CIPOS=-(\\d+,\\d+)')\n",
    "#dfFVBL['CIEND'] = dfFVBL['INFO'].str.extract(';CIEND=-(\\d+,\\d+)')\n",
    "\n",
    "#dfWTFVB = pd.read_csv(\"WTFVBLumpy.vcf\", sep='\\t', skiprows=32, usecols=[0,1,4,7], engine = 'c', dtype=object)\n",
    "#dfWTFVB = dfWTFVB[(dfWTFVB['ALT'] == '<DUP>') | (dfWTFVB['ALT'] == '<DEL>')]\n",
    "#dfWTFVB['END'] = dfWTFVB['INFO'].str.extract(';END=(\\d+)')\n",
    "#dfWTFVB['POS'] = dfWTFVB['POS'].astype(int)\n",
    "#dfWTFVB['END'] = dfWTFVB['END'].astype(int)\n",
    "#dfWTFVB['LENGTH'] = dfWTFVB['END'] - dfWTFVB['POS']\n",
    "#dfWTFVB = dfWTFVB.sort_values(by=['POS'])\n",
    "#dfWTFVB = dfWTFVB[dfWTFVB['LENGTH'] > 10000]\n",
    "\n",
    "#dfFVBD = pd.read_csv(\"FVB_NJDelly.vcf\", sep='\\t', skiprows=107, usecols=[0,1,4,6,7], engine = 'c', dtype=object)\n",
    "#dfFVBD = dfFVBD[(dfFVBD['ALT'] == '<DUP>') | (dfFVBD['ALT'] == '<DEL>')]\n",
    "#dfFVBD = dfFVBD[dfFVBD['FILTER'] == 'PASS']\n",
    "#dfFVBD['END'] = dfFVBD['INFO'].str.extract(';END=(\\d+)')\n",
    "#dfFVBD['POS'] = dfFVBD['POS'].astype(int)\n",
    "#dfFVBD['END'] = dfFVBD['END'].astype(int)\n",
    "#dfFVBD['LENGTH'] = dfFVBD['END'] - dfFVBD['POS']\n",
    "#dfFVBD = dfFVBD.sort_values(by=['POS'])\n",
    "#dfFVBD = dfFVBD[dfFVBD['LENGTH'] > 10000]\n",
    "#dfFVBD = dfFVBD[dfFVBD['LENGTH'] < 100000000]\n",
    "#dfFVBD['PE'] = dfFVBD['INFO'].str.extract(';PE=(\\d+);')\n",
    "#dfFVBD['SR'] = dfFVBD['INFO'].str.extract(';SR=(\\d+);')\n",
    "#dfFVBD['PE'] = dfFVBD['PE'].astype(int)\n",
    "#dfFVBD['SR'] = dfFVBD['SR'].fillna(0)\n",
    "#dfFVBD['SR'] = dfFVBD['SR'].astype(int)\n",
    "#dfFVBD['SU'] = dfFVBD['PE'] + dfFVBD['SR']\n",
    "#dfFVBD['SU'] = dfFVBD['SU'].astype(int)\n",
    "#dfFVBD = dfFVBD[dfFVBD['SU'] > 4]\n",
    "#dfFVBD['CIPOS'] = dfFVBD['INFO'].str.extract(';CIPOS=-(\\d+,\\d+)')\n",
    "#dfFVBD['CIEND'] = dfFVBD['INFO'].str.extract(';CIEND=-(\\d+,\\d+)')\n",
    "Myc_List = ['TA39_927','TA41_1938','WT13_842','WT13_864','WT13_1356','WT13_1445','WT13_1576','WT21_222','WT21_1066']\n",
    "#Myc_List = ['TA39_927']\n",
    "for base in Myc_List:\n",
    "    Lumpy_base = pd.read_csv(base+\"LumpyAnnotated.vcf\", sep='\\t', skiprows=37, usecols=[0,1,4,7,9,10], \\\n",
    "                             engine = 'c', dtype=object)\n",
    "# Must be loaded as engine='c' as the 'python' engine isn't natively equipped with the necessary modules\n",
    "# Next, make each dataframe something easier to read and obvious\n",
    "# If unsure how to modify data, look at the python pandas documentation\n",
    "    df1 = Lumpy_base\n",
    "# Use masking to include only columns that have deletions or duplications, no inversions or translocations\n",
    "    df1 = df1[df1['ALT'] == '<DUP>']\n",
    "# We want predicted highly impactful CNVs, so only keep those that are predicted to be have a HIGH impact\n",
    "    #df1 = df1[df1['INFO'].str.contains('HIGH')]\n",
    "# Extract the CNV end position using regular expressions\n",
    "# Check python regular expression documentation for more info\n",
    "    df1['END'] = df1['INFO'].str.extract(';END=(\\d+)')\n",
    "    df1['POS'] = df1['POS'].astype(int)\n",
    "    df1['END'] = df1['END'].astype(int)\n",
    "    df1['LENGTH'] = df1['END'] - df1['POS']\n",
    "    df1 = df1[df1['LENGTH'] > 10000]\n",
    "    #df1 = df1[df1['LENGTH'] < 100000000]\n",
    "    df1[base+'_EVIDENCE'] = df1[base+'AlignedAddedreadgroupSorted'].str.extract('./.:(\\d+):')\n",
    "    df1[base+'_EVIDENCE'] = df1[base+'_EVIDENCE'].astype(int)\n",
    "    df1['FVB_NJ_EVIDENCE'] = df1['FVB_NJ'].str.extract('./.:(\\d+):')\n",
    "    df1['FVB_NJ_EVIDENCE'] = df1['FVB_NJ_EVIDENCE'].astype(int)\n",
    "    df1 = df1[df1['FVB_NJ_EVIDENCE'] <= 0]\n",
    "    df1['TUMOR_MINUS_BACKGROUND_EVIDENCE'] = df1[base+'_EVIDENCE'] - df1['FVB_NJ_EVIDENCE']\n",
    "    df1['TUMOR_MINUS_BACKGROUND_EVIDENCE'] = df1['TUMOR_MINUS_BACKGROUND_EVIDENCE'].astype(int)\n",
    "    #df1 = df1[df1['SU'] > 4]\n",
    "    #df1['CIPOS'] = df1['INFO'].str.extract(';CIPOS=-(\\d+,\\d+)')\n",
    "    #df1['CIEND'] = df1['INFO'].str.extract(';CIEND=-(\\d+,\\d+)')\n",
    "    df1 = df1.sort_values(by=['POS'])\n",
    "    \n",
    "    #df1WTFVB = pd.merge_asof(df1, dfWTFVB, on=['POS'], by=[\"#CHROM\"], direction=\"nearest\")\n",
    "    #df1WTFVB[\"NEW_LENGTH\"] = df1WTFVB['LENGTH_x'] - df1WTFVB['LENGTH_y']\n",
    "    #df1WTFVB['NEW_LENGTH'] = df1WTFVB['NEW_LENGTH'].abs()\n",
    "    #df1WTFVB = df1WTFVB[df1WTFVB['NEW_LENGTH'] < 100]\n",
    "    #df1newnew = pd.concat([df1, df1WTFVB]).drop_duplicates(subset=['#CHROM','POS'], keep=False)\n",
    "    #df1newnew.replace('', np.nan, inplace=True)\n",
    "    #df1newnew.dropna(axis='columns', inplace=True)\n",
    "    \n",
    "    #df1FVBL = pd.merge_asof(df1, dfFVBL, on=['POS'], by=[\"#CHROM\"], direction=\"nearest\")\n",
    "    #df1FVBL[\"NEW_LENGTH\"] = df1FVBL['LENGTH_x'] - df1FVBL['LENGTH_y']\n",
    "    #df1FVBL['NEW_LENGTH'] = df1FVBL['NEW_LENGTH'].abs()\n",
    "    #df1FVBL = df1FVBL[df1FVBL['NEW_LENGTH'] < 100]\n",
    "    #df1new = df1new.reset_index()\n",
    "    #df1FVBL = df1FVBL.reset_index()\n",
    "    #df1new = pd.concat([df1, df1FVBL]).drop_duplicates(subset=['#CHROM','POS'], keep=False)\n",
    "    #df1new.replace('', np.nan, inplace=True)\n",
    "    #df1new.dropna(axis='columns', inplace=True)\n",
    "    \n",
    "    \n",
    "# Read in Delly VCF files\n",
    "    Delly_base = pd.read_csv(base+\"DellyAnnotated.vcf\", sep='\\t', skiprows=112, usecols=[0,1,4,6,7], \\\n",
    "                             engine = 'c', dtype=object)\n",
    "    df2 = Delly_base\n",
    "    df2 = df2[df2['ALT'] == '<DUP>']\n",
    "# Filter out low quality calls\n",
    "    df2 = df2[df2['FILTER'] == 'PASS']\n",
    "    #df2 = df2[df2['INFO'].str.contains('HIGH')]\n",
    "    df2['END'] = df2['INFO'].str.extract(';END=(\\d+)')\n",
    "    df2['POS'] = df2['POS'].astype(int)\n",
    "    df2['END'] = df2['END'].astype(int)\n",
    "    df2['LENGTH'] = df2['END'] - df2['POS']\n",
    "    df2 = df2[df2['LENGTH'] > 10000]\n",
    "    #df2 = df2[df2['LENGTH'] < 100000000]\n",
    "    df2['PE'] = df2['INFO'].str.extract(';PE=(\\d+);')\n",
    "    df2['SR'] = df2['INFO'].str.extract(';SR=(\\d+);')\n",
    "    df2['MAPQ'] = df2['INFO'].str.extract(';MAPQ=(\\d+);').astype(int)\n",
    "    df2['PE'] = df2['PE'].astype(int)\n",
    "    df2['SR'] = df2['SR'].fillna(0)\n",
    "    df2['SR'] = df2['SR'].astype(int)\n",
    "    df2['SU'] = df2['PE'] + df2['SR']\n",
    "    df2['SU'] = df2['SU'].astype(int)\n",
    "    df2 = df2[df2['SU'] > 4]\n",
    "    #df2['CIPOS'] = df2['INFO'].str.extract(';CIPOS=-(\\d+,\\d+)')\n",
    "    #df2['CIEND'] = df2['INFO'].str.extract(';CIEND=-(\\d+,\\d+)')\n",
    "    df2 = df2.sort_values(by=['POS'])\n",
    "    \n",
    "    #df2FVBD = pd.merge_asof(df2, dfFVBD, on=['POS'], by=[\"#CHROM\"], direction=\"nearest\")\n",
    "    #df2FVBD[\"NEW_LENGTH\"] = df2FVBD['LENGTH_x'] - df2FVBD['LENGTH_y']\n",
    "    #df2FVBD['NEW_LENGTH'] = df2FVBD['NEW_LENGTH'].abs()\n",
    "    #df2FVBD = df2FVBD[df2FVBD['NEW_LENGTH'] < 100]\n",
    "    #df2new = pd.concat([df2, df2FVBD]).drop_duplicates(subset=['#CHROM','POS'], keep=False)\n",
    "# Merge Delly and Lumpy dataframes based on Columns of interest\n",
    "# The chromosomes, position on the chromosome, and type of CNV should all match\n",
    "# There may be slight deviations on start and end position\n",
    "# You can try to build in a base pair buffer if you want, but I don't know how\n",
    "    df1['END'] = df1['END'].astype(int)\n",
    "    df2['END'] = df2['END'].astype(int)\n",
    "    df1.dropna(axis='columns', inplace=True)\n",
    "    df2.dropna(axis='columns', inplace=True)\n",
    "    df1 = df1.sort_values(by=['END'])\n",
    "    df2 = df2.sort_values(by=['END'])\n",
    "    df = pd.merge_asof(df1, df2, on=['END'], by=[\"#CHROM\",\"ALT\"], direction=\"nearest\")\n",
    "    df[\"NEW_LENGTH\"] = df['LENGTH_x'] - df['LENGTH_y']\n",
    "    df['NEW_LENGTH'] = df['NEW_LENGTH'].abs()\n",
    "    df = df[df['NEW_LENGTH'] < 100]\n",
    "    df = df[df['MAPQ'] >= 60]\n",
    "# Delly and Lumpy will give different endpoints that sometimes vary by a few base pairs\n",
    "# I just took whichever end point was the greatest and set that as the final end point\n",
    "# You can do an average end point if you want otherwise\n",
    "# Circos won't take a number other than an integer though, so be sure to round it to the nearest integer\n",
    "    df['POS'] = df[['POS_x', 'POS_y']].mean(axis=1).round().astype(int)\n",
    "    df = df[df['INFO_x'].str.contains('ANN')]\n",
    "    df['GENE_HIGH'] = df['INFO_x'].str.findall('\\|HIGH\\|(.*?)\\|').apply(','.join)\n",
    "    df['GENE_MODERATE'] = df['INFO_x'].str.findall('\\|MODERATE\\|(.*?)\\|').apply(','.join)\n",
    "    df['GENE_LOW'] = df['INFO_x'].str.findall('\\|LOW\\|(.*?)\\|').apply(','.join)\n",
    "    df['GENE_MODIFIER'] = df['INFO_x'].str.findall('\\|MODIFIER\\|(.*?)\\|').apply(','.join)\n",
    "    df['GENE'] = df['GENE_HIGH']+df['GENE_MODERATE']+df['GENE_LOW']+df['GENE_MODIFIER']\n",
    "    #df['TYPE'] = df['INFO_x'].str.extract('\\|(.*?)\\|HIGH\\|')\n",
    "    df['MM#CHROM'] = 'mm' + df['#CHROM'].astype(str)\n",
    "    df['CIRCOS_COLOR'] = 'fill_color=color'+df['ALT']\n",
    "    #df['SU_AVERAGE'] = df[['SU_x', 'SU_y']].mean(axis=1).round().astype(int)\n",
    "    df['LARGEST_EVIDENCE'] = df['TUMOR_MINUS_BACKGROUND_EVIDENCE'].max()\n",
    "    df['WEIGHT'] = df['TUMOR_MINUS_BACKGROUND_EVIDENCE'] / df['LARGEST_EVIDENCE']\n",
    "    df = df.sort_values(by=['TUMOR_MINUS_BACKGROUND_EVIDENCE'], ascending=False)\n",
    "    #df = df[df['TUMOR_MINUS_BACKGROUND_EVIDENCE'] >= 10]\n",
    "    df.loc[df['ALT'] == '<DUP>', 'NEW_WEIGHT'] = df['WEIGHT']\n",
    "    df.loc[df['ALT'] == '<DEL>', 'NEW_WEIGHT'] = df['WEIGHT'] * (-1)\n",
    "    df['CIRCOS_WEIGHT'] = 1.5\n",
    "    with pd.ExcelWriter('Myc_'+base+'_DUP.xlsx') as writer:\n",
    "        df.to_excel(writer, sheet_name='Total_DUP_Info')\n",
    "        df.to_excel(writer, sheet_name='Circos', columns=['MM#CHROM','POS','END','NEW_WEIGHT'])\n",
    "        df.to_excel(writer, sheet_name='Circos_All_DUP', columns=['MM#CHROM','POS','END','CIRCOS_WEIGHT','CIRCOS_COLOR'])\n",
    "    df['Chromosome'] = df['#CHROM']\n",
    "    df['Start Position'] = df['POS']\n",
    "    df['End Position'] = df['END']\n",
    "    df['Type of Event'] = df['ALT']\n",
    "    df['Gene(s)'] = df['GENE']\n",
    "    #df['Effect'] = df['TYPE']\n",
    "    with pd.ExcelWriter('Myc_'+base+'_DUPs_Pub.xlsx') as writer:\n",
    "        df.to_excel(writer, sheet_name='Total Info', columns=['Chromosome','Start Position','End Position','Type of Event'\\\n",
    "                                                             ,'Gene(s)'])\n",
    "# Create an empty list\n",
    "    CNV_baseList1 = pd.DataFrame()\n",
    "# Extract gene names to list and remove duplicates\n",
    "# Delly and Lumpy often call the same CNV twice or more, so remove all extra instances of a gene\n",
    "# This will be important for later when you want to count instances of a gene across multiple tumors\n",
    "    CNV_baseList1 = df['GENE'].str.split(',')\n",
    "    CNV_baseList1 = CNV_baseList1.explode().reset_index(drop=True)\n",
    "    CNV_baseList1.drop_duplicates(keep='first', inplace=True)\n",
    "    CNV_baseList1.to_csv('Myc_'+base+'_DUP_Gene_List.csv', sep='\\t')\n",
    "# Import Counter for counting instances of each gene to gain a consensus\n",
    "from collections import Counter\n",
    "total_gene_list = []\n",
    "total_gene_list.clear()\n",
    "Myc_List = ['TA39_927','TA41_1938','WT13_842','WT13_864','WT13_1356','WT13_1445','WT13_1576','WT21_222','WT21_1066']\n",
    "#Myc_List = ['3790']\n",
    "for base in Myc_List:\n",
    "# Read in csv files to a dataframe\n",
    "    dfbase = pd.read_csv('Myc_'+base+'_DUP_Gene_List.csv', usecols=[1], sep='\\t')\n",
    "    dfbase.dropna(axis='rows', inplace=True)\n",
    "# Convert dataframe to a list\n",
    "# This gets rid of a problem where I had numbers always sticking to the front of my genes\n",
    "    base_gene_list = dfbase.values.tolist()\n",
    "# Append each gene list read in through the for loop to the total gene list\n",
    "# There may be many duplicates of genes in this list, so we're going to count them\n",
    "    total_gene_list.append(base_gene_list)\n",
    "# The total gene list right now is a list of lists of lists\n",
    "# The Counter function doesn't work on lists, as lists themselves are not hashable\n",
    "# For two separate times, convert shrink down this list of lists of lists to only a list\n",
    "    flat_total_gene_list = [item for sublist in total_gene_list for item in sublist]\n",
    "    final_flat_total_gene_list = [item for sublist in flat_total_gene_list for item in sublist]\n",
    "# Call your counter of the final flat total gene list to something smaller for simplicity\n",
    "albert = Counter(final_flat_total_gene_list)\n",
    "# The counter function gives you a dictionary in python terms\n",
    "# Each gene count is assigned to each gene name\n",
    "# We want to write this to a csv file and must do so using a method specifically for dictionaries\n",
    "with open('Myc_DUP_Total_Gene_Count.csv','w') as csvfile:\n",
    "    fieldnames=['Gene', 'Count']\n",
    "    writer=csv.writer(csvfile)\n",
    "    writer.writerow(fieldnames)\n",
    "    for key, value in albert.items():\n",
    "        writer.writerow((key, value))\n",
    "# After being returned as a csvfile, there may be a couple rows with values like '0' or 'nan' which you can remove manually\n",
    "# Congrats, you now have your CNV gene names and how many tumors they were present in\n",
    "# Modify this code to your heart's content\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d2fa45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
